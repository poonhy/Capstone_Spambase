---
title: "Spambase"
author: "Poon HY"
date: "10/30/2021"
output: 
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **INTRODUCTION**

The aim of this project is to develop a model that predicts whether emails are spam, i.e. unsolicited commercial email. The model is developed from a dataset comprising of thousands of emails with a set of attributes, namely the frequency of occurrence of certain words, characters and capital letters. 

The dataset is downloaded from the UC Irvine Machine Learning repository at https://archive.ics.uci.edu/ml. The  code below downloads the dataset, separates it into two sets ("temp" and "val_set"), and further separates the temp set into a training dataset ("train_set") and testing dataset ("test_set"). The datasets **train_set** and **test_set** will be used to develop the spam detection model which will then be applied to **val_set** as a final test of the model.

```{r loading data, warning=FALSE, message=FALSE}
# Install packages as required
if(!require(tidyverse)) install.packages("tidyverse", 
                                         repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")

# Load required libraries
library(tidyverse)
library(caret)
library(e1071)

# Set global option for significant digits
options(digits = 4)

# Load data from UCI site
url_uci <- 
  "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
spambase <- read.csv(url_uci, header=FALSE)

# Insert column names
colnames(spambase) <- c("f_make", "f_address", "f_all", "f_3d", "f_our", "f_over", 
                        "f_remove", "f_internet", "f_order", "f_mail", "f_receive",
                        "f_will", "f_people", "f_report", "f_addresses", "f_free",
                        "f_business", "f_email", "f_you", "f_credit", "f_your", 
                        "f_font", "f_000", "f_money", "f_hp", "f_hpl", "f_george",
                        "f_650", "f_lab", "f_labs", "f_telnet", "f_857", "f_data", 
                        "f_415", "f_85", "f_technology", "f_1999", "f_parts", "f_pm",
                        "f_direct", "f_cs", "f_meeting", "f_original", "f_project",
                        "f_re", "f_edu", "f_table", "f_conference", 
                        "f_;", "f_(", "f_[", "f_!", "f_$", "f_3", 
                        "c_average", "c_longest", "c_total", "spam")

# Classify spam outcome as factor of 2 levels "0" (false) and "1" (true)
spambase$spam <- factor(spambase$spam, levels = c(0,1))

# Set aside 10% of dataset as validation set
set.seed(2021, sample.kind="Rounding") 
val_index <- createDataPartition(spambase$spam, times = 1, p = 0.1, list = FALSE)
temp <- spambase[-val_index,]
val_set <- spambase[val_index,]

# Split remaining data into train_set (80%) and test_set (20%)
set.seed(2021, sample.kind = "Rounding")
test_index <- createDataPartition(temp$spam, times = 1, p = 0.2, list = FALSE)
train_set <- temp[-test_index,]
test_set <- temp[test_index,]
```

So now we have 3 relevant datasets: **train_set**, **test_set** and **val_set**, which have the following number of rows:

```{r tabulate, echo=FALSE}
# Tabulate datasets and number of rows in each
data.frame(Number_of_rows = c(nrow(train_set), nrow(test_set), nrow(val_set)), 
           row.names = c("train_set", "test_set", "Validation set"))
```


# **ANALYSIS**

## **Description of data**

The first 3 rows of train_set are displayed below:

```{r head_spambase, echo=FALSE}
# First 3 rows of spambase dataset
head(spambase, 3)
```

The last column, "spam", indicates whether the email is spam ("1") or not ("0"). The first 48 columns show the frequencies of occurrence of certain words, measured in percentage of occurrence against the total number of words in the email. For example, an "f_make" value of 0.15 means that the word "make" made up 0.15% of the number of words in the email. The next 6 columns show the same, but for certain characters instead of words. Then the next 3 columns are all about capital letters - the average length of uninterrupted sequences, length of longest sequence, and total number.

The proportion of spam emails in the dataset is 39.4%, as given by the simple code below.

```{r prop_spam}
sum(spambase$spam==1)/nrow(spambase)*100
```


## **Summary statistics**

I now look at the summary statistics of the spambase dataset. For brevity, I will just show the statistics of the first 8 columns:

```{r summ_stats}
# Show summary statistics of first 6 columns of spambase dataset
summary(spambase[1:8])
```

A few words like "you" and "your", and the characters on the list appear in most of the emails, as indicated by the non-zero median occurrence. Most of the words on the list, however, do not appear in most of the emails, since the median number of occurrence is zero. 

When I plotted histograms of the frequency of 4 words in the list, you can see that the typical skew towards the zero end of the count.

```{r hist_4words}
# Plot 4 histograms in 2 rows of 2
par(mfrow = c(2,2))
hist(spambase$f_make, main = NULL, xlab = "word = 'make'")
hist(spambase$f_our, main = NULL, xlab = "word = 'our'")
hist(spambase$f_george, main = NULL, xlab = "word = 'george'")
hist(spambase$f_project, main = NULL, xlab = "word = 'project'")
```

## **Frequency of occurrence of words**

How do the distributions of key words in columns 1 to 48 look for spam and non-spam emails? Using train_set, I looked at this with the code below.

```{r plot_words_freq}
# Plot variables with high
num_spam_1 <- sum(spambase$spam==1)
# Number of non-spam emails
num_spam_0 <- sum(spambase$spam==0)
# Frequency of words per spam email
freq_w_1 <- colSums(train_set[which(train_set$spam==1), 1:48])/num_spam_1
# Frequency of words per non=spam email
freq_w_0 <- colSums(train_set[which(train_set$spam==0), 1:48])/num_spam_0
# Put bar charts of average frequencies of words for spam and non-spam emails 
# side-by-side
# Average frequency of words for spam email in descending order
order_w_1 <- freq_w_1[order(-freq_w_1)]
# Average frequency of words for non-spam email in descending order
order_w_0 <- freq_w_0[order(-freq_w_0)]
# 1 row of 2 plots
par(mfrow = c(1,2))
# Set same y-axis scales with ylim for comparability
barplot(order_w_1, ylim = c(0, 2), xlab = "Words in spam emails", 
        ylab = "Average frequency", xaxt = 'n')
barplot(order_w_0, ylim = c(0, 2), xlab = "Words in non-spam emails", 
        ylab = "Average frequency", xaxt = 'n')
```

The bars are plotted in descending order of the average frequencies of the words' occurrences. Other than the first bar ion the spam group, the distribution looks similar, though of course the order of the bars do not represent the same words in both sets. To see which bars represent which words, we zoom in to the first 5 bars of each set.

```{r plot_words_freq_5}
# Plot top 5 frequency words of spam and non-spam emails side-by-side
# 1 row of 2 plots
par(mfrow = c(1,2))
barplot(order_w_1[1:5], main = "Words in spam emails", ylim = c(0, 2), las=2, 
        ylab = "Average frequency", col = "yellow")
barplot(order_w_0[1:5], main = "Words in non-spam emails", ylim = c(0, 2), las=2, 
        ylab = "Average frequency", col = "cyan")
```

As expected, words with top occurrences in both sets are not the same, except "will" which appears in the top 5 in both.

## **Frequency of occurrence of characters**

I did the same for the occurrence of characters in columns 49 to 54. 

```{r plot_freq_char}
# Frequency of characters per spam email
freq_c_1 <- colSums(train_set[which(train_set$spam==1), 49:54])/num_spam_1
# Frequency of characters per non=spam email
freq_c_0 <- colSums(train_set[which(train_set$spam==0), 49:54])/num_spam_0
# Put bar charts of average frequencies of characters for spam and non-spam emails 
# side-by-side
# 1 row of 2 plots
par(mfrow = c(1,2))
# Set same y-axis scales with ylim for comparability
barplot(freq_c_1[order(-freq_c_1)], main = "Characters in spam emails", 
        ylim = c(0, 0.4), ylab = "Average frequency", col = "yellow", 
        cex.main = 1.1, cex.names = 0.8)
barplot(freq_c_0[order(-freq_c_0)], main = "Characters in non-spam emails",
        ylim = c(0, 0.4), ylab = "Average frequency", col = "cyan", 
        cex.main = 1.1, cex.names = 0.8)
```
Interestingly, the characters "!" and "$" appear much more often in the spam emails, which makes sense since the purpose of spam is to grab people's attention and reach into their wallets.

## **Capital letters**

Next, I looked at the 3 columns with the capital letter attributes.

```{r plot_cap}
# Capital letter occurrences per spam email
cap_1 <- colSums(train_set[which(train_set$spam==1), 55:57])/num_spam_1
# Capital letter occurrences per non=spam email
cap_0 <- colSums(train_set[which(train_set$spam==0), 55:57])/num_spam_0
# Insert a row of 3 bar charts with 3 capital letter attributes
par(mfrow = c(1,3))
# Plot of average length of uninterrupted sequences of capital letters
barplot(c(cap_1[1], cap_0[1]), main = "Average cap length", 
        names.arg = c("Spam", "Non-Spam"), col = c("yellow", "cyan"))
# Plot of avearge longest sequence of capital letters
barplot(c(cap_1[2], cap_0[2]), main = "Average longest cap length", 
        names.arg = c("Spam", "Non-Spam"), col = c("yellow", "cyan"))
# Plot of average number of capital letters
barplot(c(cap_1[3], cap_0[3]), main = "Average cap letters", 
        names.arg = c("Spam", "Non-Spam"), col = c("yellow", "cyan"))
```
This is where we see quite a stark difference between spam and non-spam emails. It seems capital letters feature quite prominently in spam emails.


# **METHODS FOR CONSTRUCTING PREDICTION MODELS**

## **Accuracy function**

To measure the accuracy of the predictions, I defined the function below:

```{r accuracy}
# Accuracy function
accuracy <- function(predicted_spam, true_spam){
  confusionMatrix(predicted_spam, true_spam)$overall["Accuracy"]
}
```

## **Just guessing**

In the most basic model, I simply guessed the outcome with equal probability of predicting whether an email is spam or not. 

```{r guess, warning=FALSE, message=FALSE}
# Just guessing
set.seed(2021, sample.kind = "Rounding")
# Generate guesses for all observations in test_set
pred_guess <- as.factor(sample(c(0,1), nrow(test_set), replace = TRUE))
# Check accuracy
accuracy_guess <- accuracy(pred_guess, test_set$spam)
accuracy_guess
```
The accuracy of just guessing was about 50%, as expected.

## **Naive Bayes**

The first machine learning algorithm I used for prediction was the Naive Bayes classifier, which is based on Bayes Theorem, and known to be fast. I split train_set into the outcome ("y") and all other columns which are the predictors ("x").

```{r naive_bayes, warning=FALSE, message=FALSE}
# Select "spam" column (no. 58) and place into y and place rest of columns into x
y <- train_set[, 58]
x <- train_set[, -58]

# Naive Bayes
# Obtain fit by training using Naive Bayes method
train_nb <- train(x, y, method = "naive_bayes")
# Calculate predictions using Naive Bayes fit
pred_nb <- predict(train_nb, test_set)
# Check accuracy
accuracy_nb <- accuracy(pred_nb, test_set$spam)
accuracy_nb
```
The accuracy for Naive Bayes was obviously a significant improvement over just guessing, but not very high.

## **Logistic regression**

Next, I looked at logistic regression. The resulting accuracy was a significant improvement over Naive Bayes.

```{r glm, warning=FALSE, message=FALSE}
# Logistic regression
# Obtain fit by training using GLM method
train_glm <- train(x, y, method = "glm")
# Calculate predictions using GLMs fit
pred_glm <- predict(train_glm, test_set, type = "raw")
# Check accuracy
accuracy_glm <- accuracy(pred_glm, test_set$spam)
accuracy_glm
```

## **Support Vector Machine (SVM)**

I then used a algorithm called Support Vector Machine (SVM), which tries to find a hyperplane that distinctly classifies data points. 

```{r svm}
# Support Vector Machine (SVM)
# Obtain fit by training using SVM method
train_svm <- svm(spam ~ ., kernel = "linear", data = train_set)
# Calculate predictions using SVM fit
pred_svm <- predict(train_svm, test_set, type = "raw")
# Check accuracy
accuracy_svm <- accuracy(pred_svm, test_set$spam)
accuracy_svm
```
There was a slight improvement, and I noticed it was fast.

## **K-Nearest Neighbours (KNN)**

Next up was the use of the K-Nearest Neighbours (KNN) algorithm.

```{r knn, warning=FALSE, message=FALSE}
# K-Nearest Neighbours or KNN
set.seed(2021, sample.kind = "Rounding")
# Obtain fit by training using KNN method
train_knn <- train(x, y, method = "knn")
# Calculate predictions using KNN fit
pred_knn <- predict(train_knn, test_set, type = "raw")
# Check accuracy
accuracy_knn <- accuracy(pred_knn, test_set$spam)
accuracy_knn
```
The resulting accuracy was actually quite low. Computation also took some time.

## **Random Forest**

Finally, I applied the Random Forest algorithm to train_set. 

```{r rf, warning=FALSE, message=FALSE}
# Random forest
set.seed(2021, sample.kind = "Rounding")
# Obtain fit by training using Random Forest (RF) method
train_rf <- train(spam ~ ., method = "rf", data = train_set)
# Calculate predictions using RF fit
pred_rf <- predict(train_rf, test_set, type = "raw")
# Check accuracy
accuracy_rf <- accuracy(pred_rf, test_set$spam)
accuracy_rf
```
Accuracy was good, although computation time was the longest among the algorithms that I used.

# **RESULTS**

The various results are tabulated below:

```{r acc_results, echo=FALSE}
#Tabulate results
results <- data.frame(Method=c("Just guess", "Naive Bayes", "GLM", "SVM", "KNN", 
                               "RandomForest"), 
                      Accuracy=c(accuracy_guess, accuracy_nb, accuracy_glm,
                                 accuracy_svm, accuracy_knn, accuracy_rf))
results
```

Accuracy is not the only relevant measure for a spam email predictor, however. A model that throws up a high false positive rate is also bothersome as it means a high number of legitimate emails would be flagged as spam and possibly dumped into a spam folder. Hence, I calculated the false positive rates for each method and tabulated the results together with accuracy.

```{r results_with_false_positive}
# # Calculate false positive rates of the various methods
preds <- list(pred_guess, pred_nb, pred_glm, pred_svm, pred_knn, pred_rf)
false_pos <- sapply(preds, function(pred){
  sum(pred==1 & test_set$spam==0)/nrow(test_set)
})

#Tabulate results with false positive rates
results_fp <- data.frame(Method=c("Just guess", "Naive Bayes", "GLM", "SVM", 
                                  "KNN", "RandomForest"), 
                         Accuracy=c(accuracy_guess, accuracy_nb, accuracy_glm,
                                 accuracy_svm, accuracy_knn, accuracy_rf),
                         False_positive=c(false_pos[1:6]))
results_fp
```

Although computation time was longest for the **Random Forest** method, it delivered the highest accuracy and lowest false positive rate. I therefore chose this method to work on the validation set.

```{r val_accuracy}
# Use Random Forest on validation set
pred_val_rf <- predict(train_rf, val_set, type = "raw")
# Check accuracy
accuracy_val_rf <- accuracy(pred_val_rf, val_set$spam)
accuracy_val_rf
```
The resulting accuracy of applying Random Forest to the validation set was 0.95, close to that for test_set.

I also checked that the false positive rate was acceptable.

```{r val_falsepos}
# Check false positive rate
falsepos_val_rf <- sum(pred_val_rf==1 & val_set$spam==0)/nrow(val_set)
falsepos_val_rf
```


# **CONCLUSION**

The quality of the predictor model obviously depends on the quality of the data and the choice of data attributes. In this case, since the choice of words and characters to include in the dataset is discretionary, there is a risk that the data collectors have chosen the wrong words and characters, or not picked the right ones. Fortunately, the dataset has delivered quite an accurate model, so it appears the attributes have been well chosen.

Given that spam emails are created by people keen to evade spam detectors, such emails might be tweaked over time to render the detector less effective. Thus, data would need to be regularly collected and  reviewed for relevance, and the model itself adjusted to thwart spammers.

In terms of method, I decided to use the Random Forest algorithm on the validation set as it had delivered the most accurate result and lowest false positive rate on test_set. One limitation of the Random Forest method is the long computation time, in fact the longest amongst the methods tried. If the dataset was much larger, I might have used the Support Vector Machine (SVM) method which was close to the accuracy to Random Forest and had a low false positive rates, but much faster.

